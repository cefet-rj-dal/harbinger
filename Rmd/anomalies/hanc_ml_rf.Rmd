This tutorial shows supervised anomaly detection with a Random Forest classifier on a labeled train/test split. Steps:

- Load and visualize the dataset
- Normalize, train RF, evaluate on train and test
- Plot detections and residual magnitudes

```{r}
# Install Harbinger (if needed)
#install.packages("harbinger")
```

```{r}
# Load required packages
library(daltoolbox)
library(harbinger) 
```

```{r}
# Load example anomaly datasets
data(examples_anomalies)
```

```{r}
# Select the train/test dataset
dataset <- examples_anomalies$tt

head(dataset)
```

```{r}
# Plot the raw time series
har_plot(harbinger(), dataset$serie)
```

```{r}
# Split into train/test and normalize features
train <- dataset[1:80,]
test <- dataset[-(1:80),]

norm <- minmax()
norm <- fit(norm, train)
train_n <- transform(norm, train)
summary(train_n)
```

```{r}
# Configure RF classifier
model <- hanc_ml(cla_rf("event", c("FALSE", "TRUE"), mtry = 1, ntree = 5))
```

```{r}
# Fit on training data and evaluate on train
model <- fit(model, train_n)
detection <- detect(model, train_n)
print(detection |> dplyr::filter(event == TRUE))
evaluation <- evaluate(model, detection$event, as.logical(train_n$event))
print(evaluation$confMatrix)
```

```{r}
# Plot training detections
har_plot(model, train_n$serie, detection, as.logical(train_n$event))
```

```{r}
# Prepare normalized test set
test_n <- transform(norm, test)
```

```{r}
# Detect and evaluate on test
detection <- detect(model, test_n)
print(detection |> dplyr::filter(event == TRUE))
evaluation <- evaluate(model, detection$event, as.logical(test_n$event))
print(evaluation$confMatrix)
```

```{r}
# Plot test detections
har_plot(model, test_n$serie, detection, as.logical(test_n$event))
```

```{r}
# Plot residual magnitude and decision threshold
har_plot(model, attr(detection, "res"), detection, test_n$event, yline = attr(detection, "threshold"))
```
